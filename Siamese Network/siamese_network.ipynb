{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanSamanek/Project_Shaman/blob/centroid-method/Siamese%20Network/siamese_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1APTmeH7mp-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfexY75_tjEf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRuB5rZ3tjEj"
      },
      "source": [
        "## Prepare Data For Siamese Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZptjIuZPtjEk"
      },
      "source": [
        "Load Data and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8W5uU5ftjEl"
      },
      "outputs": [],
      "source": [
        "train_data_dir = r'/content/drive/MyDrive/bounding_box_train'\n",
        "val_data_dir = r'/content/drive/MyDrive/bounding_box_val'\n",
        "\n",
        "file_paths_train = tf.data.Dataset.list_files(train_data_dir + '/*.jpg')\n",
        "file_paths_val = tf.data.Dataset.list_files(val_data_dir + '/*.jpg')\n",
        "\n",
        "# Define the function to extract the label from the file name\n",
        "# works for my specific directory path...\n",
        "def extract_label(file_path):\n",
        "    label = tf.strings.split(file_path, '_')\n",
        "    label = tf.strings.split(label, '/')\n",
        "    return int(label[2][1])\n",
        "\n",
        "def read_and_decode(file_path):\n",
        "    label = extract_label(file_path)\n",
        "    image = tf.io.read_file(file_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    return image, label\n",
        "\n",
        "print(\"[INFO] loading data...\")\n",
        "\n",
        "dataset_train = [read_and_decode(file) for file in file_paths_train]\n",
        "dataset_val = [read_and_decode(file) for file in file_paths_val]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0xBgF9VtjEm"
      },
      "source": [
        "Visualize Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYUDSMoUtjEn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "subplot_pos = 1\n",
        "\n",
        "for image, label in dataset_train[:9]:\n",
        "  ax = plt.subplot(1, 9, subplot_pos)\n",
        "  subplot_pos += 1\n",
        "  plt.imshow(image)\n",
        "  plt.title(int(label))\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "print(\"IMAGE SHAPE: \", image.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWUp3WaXtjEo"
      },
      "source": [
        "Make Pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkocPPiAtjEo"
      },
      "outputs": [],
      "source": [
        "def make_pairs(dataset):\n",
        "    pairs_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    print(\"[INFO] preparing positive and negative pairs...\")\n",
        "\n",
        "    images = [image for image, label in dataset]\n",
        "    labels = [label for image, label in dataset]\n",
        "\n",
        "    unique_labels = np.unique(np.array(labels))\n",
        "\n",
        "    idxs = [np.where(labels == unique_label)[0] for unique_label in unique_labels]\n",
        "    for idx_1 in range(len(labels)):\n",
        "        label = labels[idx_1]\n",
        "        img_1 = images[idx_1]\n",
        "        # randomly pick an image that belongs to the *same* class\n",
        "        for i in range(2):\n",
        "            idx_2 = np.random.choice(np.where(np.array(labels) == label)[0])\n",
        "            img_2 = images[idx_2]\n",
        "            pairs_list.append((img_1, img_2))\n",
        "            labels_list.append([1])\n",
        "\n",
        "        # randomly pick an image that does *not* belong to the same class\n",
        "        for i in range(2):\n",
        "            idx_2 = np.random.choice(np.where(np.array(labels) != label)[0])\n",
        "            img_2 = images[idx_2]\n",
        "            pairs_list.append((img_1, img_2))\n",
        "            labels_list.append([0])\n",
        "\n",
        "    return np.array(pairs_list), np.array(labels_list)\n",
        "\n",
        "train_pair_x, train_pair_y = make_pairs(dataset_train)\n",
        "val_pair_x, val_pair_y = make_pairs(dataset_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt9uDjf8tjEo"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 9)\n",
        "plt.subplots_adjust(wspace=0.5, hspace=0)\n",
        "j = 0\n",
        "\n",
        "for img_1, img_2 in train_pair_x[:9]:\n",
        "  axes[0, j].imshow(img_1)\n",
        "  axes[1, j].imshow(img_2)\n",
        "  axes[0, j].set_title(train_pair_y[j])\n",
        "  axes[0, j].set_axis_off()\n",
        "  axes[1, j].set_axis_off()\n",
        "  j += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1n7T-zqtjEp"
      },
      "source": [
        "## Creating Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxnXhEUctjEp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3G9SQqptjEq"
      },
      "source": [
        "Choose a Pretrained Model as Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQXCPSK5tjEq"
      },
      "outputs": [],
      "source": [
        "# the image size for all the images in The Market Dataset is 128x64\n",
        "IMG_SHAPE = (128, 64, 3)\n",
        "\n",
        "# Create the base model from the pre-trained model MobileNet V2\n",
        "feature_extractor = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "feature_extractor.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Vsot73JtjEq"
      },
      "outputs": [],
      "source": [
        "feature_extractor.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acfbSM5LtjEq"
      },
      "source": [
        "Create similarity evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3PtMCj_tjEq"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def euclidean_distance(vectors):\n",
        "\t(features_1, features_2) = vectors\n",
        "\t# compute the sum of squared distances between the vectors\n",
        "\tsumSquared = K.sum(K.square(features_1 - features_2), axis=1, keepdims=True)\n",
        "\t# return the euclidean distance between the vectors\n",
        "\treturn K.sqrt(K.maximum(sumSquared, K.epsilon()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-LQ8_wStjEr"
      },
      "source": [
        "Configure Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg91p8T4tjEr"
      },
      "outputs": [],
      "source": [
        "print(\"[INFO] building model...\")\n",
        "img_1 = Input(shape=IMG_SHAPE)\n",
        "img_2 = Input(shape=IMG_SHAPE)\n",
        "\n",
        "features_1 = feature_extractor(img_1)\n",
        "features_2 = feature_extractor(img_2)\n",
        "distance = Lambda(euclidean_distance)([features_1, features_2])\n",
        "pooling = GlobalAveragePooling2D()(distance)\n",
        "dropout = Dropout(0.2)(pooling)\n",
        "dense = Dense(64, activation=\"relu\")(dropout)\n",
        "outputs = Dense(1, activation=\"sigmoid\")(dense)\n",
        "model = Model(inputs=[img_1, img_2], outputs=outputs)      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i30sF0vdtjEr"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6XxAlXRtjEr"
      },
      "source": [
        "## Training Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poNdU6zutjEr"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "initial_epochs = 20\n",
        "\n",
        "# compile the model\n",
        "print(\"[INFO] compiling model...\")\n",
        "base_learning_rate = 0.0001\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate),\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "history = model.fit(\n",
        "\t[train_pair_x[:, 0], train_pair_x[:, 1]], train_pair_y[:],\n",
        "\tvalidation_data=([val_pair_x[:, 0], val_pair_x[:, 1]], val_pair_y[:]),\n",
        "\tbatch_size=BATCH_SIZE, \n",
        "\tepochs=initial_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN6yZ1YSwV3N"
      },
      "source": [
        "Fine Tune Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkSoKZ-RwOmM"
      },
      "outputs": [],
      "source": [
        "feature_extractor.trainable = True\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in feature_extractor.layers[:fine_tune_at]:\n",
        "  layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGtXykevwl06"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "# compile the model\n",
        "print(\"[INFO] compiling model...\")\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "fine_tune_epochs = 20\n",
        "total_epochs =  initial_epochs + fine_tune_epochs\n",
        "\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "history_fine = model.fit(\n",
        "\t[train_pair_x[:, 0], train_pair_x[:, 1]], train_pair_y[:],\n",
        "\tvalidation_data=([val_pair_x[:, 0], val_pair_x[:, 1]], val_pair_y[:]),\n",
        "\tbatch_size=BATCH_SIZE,\n",
        "  initial_epoch=history.epoch[-1], \n",
        "\tepochs=total_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZL3Cp9LtjEr"
      },
      "source": [
        "Visualize Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ysm0yW7tjEs"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "acc += history_fine.history['accuracy']\n",
        "val_acc += history_fine.history['val_accuracy']\n",
        "\n",
        "loss += history_fine.history['loss']\n",
        "val_loss += history_fine.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.plot([initial_epochs-1,initial_epochs-1],\n",
        "          plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.ylim([0, 1.0])\n",
        "plt.plot([initial_epochs-1,initial_epochs-1],\n",
        "         plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n2fEfn5tjEs"
      },
      "source": [
        "Save Siamese Network Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh0UADWFtjEs"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/model/siamese_network.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwJX3zuCtjEs"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufDDEL_YtjEs"
      },
      "source": [
        "Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw_7_1sZtjEs"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('content/model/siamese_network.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVw3qWfwtjEs"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict([val_pair_x[:,0], val_pair_x[:,1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFEWRuNltjEt"
      },
      "outputs": [],
      "source": [
        "for i in range(40):\n",
        "  # initialize the figure\n",
        "  fig = plt.figure(\"Pair #{}\".format(i + 1), figsize=(4, 2))\n",
        "  plt.suptitle(\"Similarity: {:.2f}\".format(predictions[i][0]))\n",
        "\n",
        "  ax = fig.add_subplot(1, 2, 1)\n",
        "  plt.imshow(val_pair_x[i, 0])\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "  ax = fig.add_subplot(1, 2, 2)\n",
        "  plt.imshow(val_pair_x[i, 1])\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_TGhANVK0K-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "predictions = tf.round(predictions)\n",
        "print(multilabel_confusion_matrix(val_pair_y, predictions))\n",
        "\n",
        "test_loss, test_acc = model.evaluate([val_pair_x[:,0], val_pair_x[:,1]], val_pair_y)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "py_3_9_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "412e3301929c0b4e6dbd1b029bf68da7d37f7d3790d9ed7946c87cf9399551e7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}